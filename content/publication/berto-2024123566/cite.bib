@article{BERTO2024123566,
 abstract = {Over the years, several neural network architectures have been proposed to process and represent texts using dense vectors (known as word embeddings): mathematical representations that encode the meaning of words or phrases. Word embeddings can be computed by many different algorithms, usually trained on large amounts of textual data aiming to capture semantic relationships between words. These embeddings revolutionized many Natural Language Processing applications, enabling more accurate and nuanced language understanding. Recently, it was demonstrated that it is possible to employ word embeddings to uncover latent knowledge, i.e., information that may be implicit in a set of texts and that would hardly be perceptible to humans. In this context, this study extends such strategy by combining different unsupervised models to accelerate discoveries in medicine. Our word embeddings were trained on a large corpus of medical papers related to Acute Myeloid Leukemia, a highly malignant form of cancer. Our study shows that established therapies could have been developed before their first proposal due to treatment testing notifications issued by our system up to 11 years in advance. The results show the potential of uncovering latent knowledge from the biomedical field to empower faster and more efficient drug testing for medical discoveries.},
 author = {Matheus V. V. Berto and Breno L. Freitas and Carolina Scarton and Jo√£o A. Machado-Neto and Tiago A. Almeida},
 doi = {https://doi.org/10.1016/j.eswa.2024.123566},
 issn = {0957-4174},
 journal = {Expert Systems with Applications},
 keywords = {Distributed vector representations, Word embeddings, Knowledge discovery in databases, Natural language processing, AI in medicine},
 pages = {123566},
 title = {Accelerating discoveries in medicine using distributed vector representations of words},
 url = {https://www.sciencedirect.com/science/article/pii/S0957417424004317},
 volume = {250},
 year = {2024}
}
